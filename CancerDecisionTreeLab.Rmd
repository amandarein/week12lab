---
title: "Decision Tree Lab"
author: "Amanda Rein, Belen Gomez Grimaldi, Kay Mattern"
date: "4/28/2021"
output:
  html_document:
    toc: TRUE
    theme: journal
    toc_float: TRUE
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r knitr_options, include=FALSE}
set.seed(1982)
```

```{r load-packages, include=FALSE}
#install.packages("rio")
library(rio)
#install.packages("plyr")
library(plyr)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("rpart")
library(rpart)
#install.packages("psych")
library(psych)
#install.packages("pROC")
library(pROC)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("rattle")
library(rattle)
# setwd("/cloud/project/decision_trees")
#install.packages("caret")
#install.packages("caret")
library(caret)
#install.packages("C50")
#install.packages("mlbench")
#install.packages("naniar")
library(naniar)
#install.packages("e1071")
library(e1071)
```

## Cleaning the Data
In order to prepare the data for decision tree training, we first removed ER.Status because it contains essentially the same information as PR.Status. Next, we removed Days.to.date.of.Death because, as the missing variable plot shows, this variable was missing for a lot of patients. Additionally, we dropped any cases after that that were not complete. 

We thought that this would be enough, but we soon learned that there were some columns that had an imbalance of information, causing our models to not build correctly. We had two solutions for this problem. First, we dropped the Gender, Metastasis, and Metastasis.Coded columns because they did not provide very much information because only a couple of patients had different values. Also, we collapsed the Stage columns (AJCC.Stage and Converted.Stage) so that the information in those columns was more balanced while still maintaining the factors. To do this, we put, for example, all Stage I, Stage IA, Stage IB variables into one category. Finally, we dropped any patients that contained unique values in the columns because if they ended up in the test set, then we would run into issues introducing new values into the decision tree.

After that, we were ready to start creating our trees! We broke down the remaining 102 patients into test and training sets.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
#1 Load the data and ensure the column names don't have spaces, hint check.names.  
cancer <- tibble(import("clinical_breast_cleaned.csv", check.names= TRUE))
gg_miss_var(cancer)
cancer <- select(cancer, -c(ER.Status, Days.to.date.of.Death)) # ER.Status same as PR.Status, Days.to.date.of.Death has lots of missing values, rest has new levels in predict
# gg_miss_var(cancer)
cancer <- cancer[complete.cases(cancer), ]

cancer$AJCC.Stage <- fct_collapse(cancer$AJCC.Stage,
                               I = c("Stage I", "Stage IA", "Stage IB"),
                               II =c("Stage II", "Stage IIA", "Stage IIB"),
                               III = c("Stage III", "Stage IIIA", "Stage IIIB", "Stage IIIC"),
                               IV = c("Stage IV"))
cancer$Converted.Stage <- fct_collapse(cancer$Converted.Stage,
                               None = c("No_Conversion"),
                               I =c("Stage I"),
                               II = c("Stage IIA", "Stage IIB"),
                               III = c("Stage IIIA", "Stage IIIB", "Stage IIIC"))


cancer <- cancer[-c(1, 7,8)]
cancer <- cancer[cancer$HER2.Final.Status != "Equivocal", ]
cancer <- cancer[cancer$AJCC.Stage != "IV", ]

cancer$HER2.Final.Status <- as.factor(cancer$HER2.Final.Status)
cancer$PR.Status <- as.factor(cancer$PR.Status)


set.seed(1982)
cancer_train_rows = sample(1:nrow(cancer),
                              round(0.8 * nrow(cancer), 0),
                              replace = FALSE)
# Check to make sure we have 80% of the rows

training = cancer[cancer_train_rows, ]
# Rows not used in training set, aka the test set
test = cancer[-cancer_train_rows, ]
```

## Analysis for PR.Status and Tumor {.tabset}

After cleaning our data, we began our analysis on both the PR.Status variable and the Tumor variable. Since PR.Status is a binary variable, we chose to run a CART-style analysis to create a decision tree. On the other hand, Tumor is a multi-class variable, so a C5.0 analysis makes more sense. 

### PR.Status {.tabset}

#### RPart (CART style)
CART analysis tends to be more useful for binary predictions, so we think that this type of tree model will work best for the PR.Status variable.

##### Base Rate
The base rate is 51.9% for identifying whether PR.Status is positive or not. 
```{r, message=FALSE, echo = FALSE, warning=FALSE}
cancer$PR.Status <- as.numeric(cancer$PR.Status)
(x <- 1- sum(cancer$PR.Status)/length(cancer$PR.Status)) # 48.57% have biomarker for PR.Status
# (x <- 1- sum(cancer$PR.Status)/length(cancer$PR.Status)) # 48.57% have biomarker for PR.Status

split <- table(cancer$PR.Status)[2] / sum(table(cancer$PR.Status))
# split
```

##### Building the Model
To build the model, we used the RPART to create a tree. From this tree, we were able to identify the most important variables (the relative importance of each variable can be seen in the table below this tree graph). In this case, OS.Time was the most important variable for the improvement of all nodes in which the attribute is a splitter. However, at the top of the tree is Age.at.Initial.Pathologic.Diagnosis, which means that Age.at.Initial.Pathologic.Diagnosis is the variable that most definitively partitions the data set to identify positive or negative PR.Status values. 

Additionally, we looked at the conditional probability table. From that, we gathered that we probably was to have a tree with 2 splits to reduce the xerror (cross-validated error) as much as possible. This can be seen in the graph below, where 2 is the leftmost value below the dotted line (representing the highest cross-validated error minus the minimum cross-validated error, plus the standard deviation of the error at that tree).
```{r, message=FALSE, echo = FALSE, warning=FALSE}
#7 Build your model using the default settings
set.seed(1981)
tree_prstatus = rpart(PR.Status~.,  #<- formula, response variable ~ predictors
                           #   "." means "use all other variables in data"
                            method = "class",#<- specify method, use "class" for tree
                            parms = list(split = "gini"),#<- method for choosing tree split
                            data = training,#<- data used
                            control = rpart.control(cp=.01))

```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#8 View the results, what is the most important variable for the tree?

tree_prstatus # root, Days.to.date.of.Last.Contact
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#9 Plot the tree using the rpart.plot package (CART only).
rpart.plot(tree_prstatus, type =4, extra = 101)#package rpart.plot
#export this to  pdf for better viewing

#The "cptable" element includes the optimal prunning based on the complexity parameter.
#View(tree_prstatus$cptable)
#The "cptable" element includes the optimal pruning based on the complexity parameter.
cp <- tree_prstatus$cptable


# table(training[,2])
# table(test[,2])
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#10 plot the cp chart and note the optimal size of the tree (CART only).

most_imp_vars <- tree_prstatus$variable.importance
most_imp_vars

plotcp(tree_prstatus) 

cptable_ex <- as_tibble(tree_prstatus$cptable)
```

##### Prediction and Error Analysis

In this section, we used the tree that we created to predict the target variable, PR.Status, for each patient in the test set.

After running the model on our test set, we then generated the confusion matrix to compare the results to the actual data. We have an accuracy of 55%, which is slightly better than our base rate. Our ROC plot below confirms this, as the line plotted is only slightly above the y = x line. Our error rate was 9 out of 20 test cases, which is certainly not ideal. Our sensitivity and specificity are also poor, meaning we will have more false positives and false negatives than we would like to. Our kappa is incredibly low too, which indicates unstable predictions. 
```{r, message=FALSE, echo = FALSE, warning=FALSE}
#11 Use the predict function and your tree to predict the target variable using test set. 
# tree_prstatus$frame

tree_predict = predict(tree_prstatus,test, type= "class")

#View(as.data.frame(tree_predict))

tree_predict <- as.numeric(tree_predict)
#View(tree_predict)
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#12 Generate, "by-hand", the hit rate and detection rate and compare the 
#detection rate to your original baseline rate. How did your models work?

tree_predict <- ifelse(tree_predict >= 2,1,0)

#install.packages("e1071")
#library(e1071)
par_conf_matrix <- confusionMatrix(as.factor(tree_predict), as.factor(test$PR.Status), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
par_conf_matrix

conf_matrix <- par_conf_matrix$table
# conf_matrix

# The error rate is defined as a classification of "Parent" when 
# this is not the case, and vice versa. It's the sum of all the
# values where a column contains the opposite value of the row.
error_rate = sum(conf_matrix[row(conf_matrix)!= col(conf_matrix)])
# 10


# The error rate divides this figure by the total number of data points
# for which the forecast is created.
# sum(conf_matrix)

# Let's use these values in 1 calculation.
par_error_rate = sum(conf_matrix[row(conf_matrix) != col(conf_matrix)]) / sum(conf_matrix)

# paste0("Hit Rate/True Error Rate:", par_error_rate * 100, "%")

#Detection Rate is the rate at which the algo detects the positive class in proportion to the entire classification A/(A+B+C+D) where A is poss correctly predicted
#par_conf_matrix
# conf_matrix

# conf_matrix[2,2]/sum(conf_matrix)

# table(cancer$PR.Status)

#We can adjust using a if else statement and the predicted prob

tree_example_prob = predict(tree_prstatus,test, type= "prob")
#View(tree_example_prob)

#Let's 

# tree_example_prob[,"0"]
roc(test$PR.Status, ifelse(tree_example_prob[,"0"] >= .50,0,1), plot=TRUE)
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#13 Use the the confusion matrix function in caret to 
#check a variety of metrics and comment on the metric that might be best for 
#each type of analysis.  

#confusionMatrix(as.factor(tree_predict), as.factor(test$PR.Status), positive = "PR.Status", dnn=c("Prediction", "Actual"), mode = "sens_spec")
con_PR_RPART = confusionMatrix(as.factor(tree_predict), as.factor(test$PR.Status), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#14 Generate a ROC and AUC output, interpret the results

# par_roc <- roc(test$PR.Status, as.numeric(tree_predict), plot = TRUE) #Building the evaluation ROC and AUV using the predicted and original target variables 

# par_roc

# plot(par_roc)
```

#### Caret (C5.0 style)

Caret analyses tend to be better suited for multi-factor or continuous classifications, so we assume that this type of modelling will be worst for identifying the PR.Status variable.

##### Creating Tree

Using the cross validation process and tuning, we found the best combination of attributes for our tree. This ended up being only one trial and winnowing. However, most models seemed to have a very similar accuracy by trial number, as can be seen on the accuracy versus trial plots below. Winnowing seemed to improve accuracy substantially, though.

We ended up having to remove more columns from our data because they were causing trouble in the algorithm due to undefined cases in the test set. Because of this, we were down to only four variables.

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#install.packages("C50")
library(C50) #Need this to pass into caret 
#install.packages("mlbench")
library(mlbench)

#Cross validation process 

fitControl <- trainControl(method = "repeatedcv",
  number = 10,
  repeats = 5, returnResamp="all") #setting up our cross validation

# number - number of folds
# repeats - number of times the cv is repeated, here it's 5 take the average of
# those 5 repeats

# Choose the features and classes

#View(training)
# View(training)

training$PR.Status <- as.factor(training$PR.Status)
test$PR.Status <- as.factor(test$PR.Status)

grid <- expand.grid(.winnow = c(TRUE,FALSE), .trials=c(1,5,10,15,20), .model="tree" )

# grid <- as.data.frame(grid)

#expand.grid - function in caret that will essentially conduct a hyper-parameter 
# and select the best options
# expand.grid(grid)

#winnow - whether to reduce the feature space - uses a regulator/penalty
#trails - number of boosting iterations to try, 1 indicates a single model 
#model - type of ml model

features <- as.data.frame(training[,c(-2)])
features <- features[, -c(2,3,4,5,6,7,8)]

target <- as.factor(training$PR.Status)

options(warn = -1)
PR_mdl <- train(x=features,y=target,tuneGrid=grid,trControl=fitControl,method="C5.0",verbose=TRUE)

PR_mdl

# str(PR_mdl$pred)

# visualize the re-sample distributions
xyplot(PR_mdl,type = c("g", "p", "smooth"))

#varImp(PR_mdl)
```

##### Analyzing Results

As it can be seen in the confusion matrix below, our results are beginning to look promising. We never had any false negatives, which is a good thing. This is reflected in our perfect Specificity score. Our accuracy was 50%, which is slightly lower than our base rate. Further, our kappa value is 0, which states zero inter-rater reliability. Our sensitivity is also 0, which means that we have a huge issue with false positives.

The overall error rate was 50%, which is bad. The area under the curve of our POC plot is 50%, which indicates that we are performing as well as random guessing, making this model particularly useless.

```{r, message=FALSE, echo = FALSE, warning=FALSE}
PRC50_predict = predict(PR_mdl,test, type="raw")

#Lets use the confusion matrix

conf_matrix_PRC50 <- confusionMatrix(as.factor(PRC50_predict), as.factor(test$PR.Status), 
                dnn=c("Prediction", "Actual"), mode = "sens_spec")


conf_matrix_PRC50
# table(test$PR.Status)


PRC50_predict_p = predict(PR_mdl,test, type= "prob")



cm_PRC50 <- conf_matrix_PRC50$table
cm_PRC50

# The error rate is defined as a classification of "Parent" when 
# this is not the case, and vice versa. It's the sum of all the
# values where a column contains the opposite value of the row.
error_rate = sum(cm_PRC50[row(cm_PRC50)!= col(cm_PRC50)])

# 4


# The error rate divides this figure by the total number of data points
# for which the forecast is created.
# sum(conf_matrix)

# Let's use these values in 1 calculation.
PRC50_error_rate = sum(cm_PRC50[row(cm_PRC50) != col(cm_PRC50)]) / sum(cm_PRC50)
# PRC50_error_rate


PR50ROC <- roc(test$PR.Status, as.numeric(PRC50_predict), plot = TRUE)

PR50ROC



# PR_predict_p

```

### Multi-Class Prediction: Tumors {.tabset}

Next, we will build another decision tree to predict the type of tumor a patient has. There are four classes of tumors (T1, T2, T3, T4).

#### RPART (CART style)
```{r, message=FALSE, echo = FALSE, warning=FALSE}
#15 Follow the same steps for the multi-class target, tumor, aside from step 1,
# 2 and 14. For step 13 compare to the four base rates and see how you did.
cancer2 <- tibble(import("clinical_breast_cleaned.csv", check.names= TRUE))
cancer2 <- select(cancer2, -c(ER.Status, Days.to.date.of.Death)) #Days.to.date.of.Death has lots of missing values, rest has new levels in predict
cancer2 <- cancer2[complete.cases(cancer2), ]
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
# Split into train/test

set.seed(1982)
#cancer_train_rows = sample(1:nrow(cancer),
                              #round(0.8 * nrow(cancer), 0),
                              #replace = FALSE)


#training2 = cancer[cancer_train_rows, ]
#test2 = cancer[-cancer_train_rows, ]

split <- createDataPartition(cancer2$Tumor,times=1,p = 0.8,list=FALSE)
training2 <- cancer2[split,]
test2 <- cancer2[-split,]
```

##### Base Rates

The base rates for the four tumor classes are:

- T1 = 14.29%

- T2 = 61.90%

- T3 = 18.10%

- T4 = 5.71%

```{r, message=FALSE, include = FALSE, warning=FALSE}
# Calculate Baserates
table(cancer2$Tumor)

t1_base <- sum(cancer2$Tumor == "T1") / length(cancer2$Tumor) * 100 # 14.29%
t2_base <- sum(cancer2$Tumor == "T2") / length(cancer2$Tumor) * 100 # 61.90%
t3_base <- sum(cancer2$Tumor == "T3") / length(cancer2$Tumor) * 100 # 18.10%
t4_base <- sum(cancer2$Tumor == "T4") / length(cancer2$Tumor) * 100 # 5.71%

# t1_base <- sum(cancer2$Tumor == "T1") / length(cancer2$Tumor) # 14.29%
# t2_base <- sum(cancer2$Tumor == "T2") / length(cancer2$Tumor) # 61.90%
# t3_base <- sum(cancer2$Tumor == "T3") / length(cancer2$Tumor) # 18.10%
# t4_base <- sum(cancer2$Tumor == "T4") / length(cancer2$Tumor) # 5.71%

```

##### Building the Multi-Class CART Model
We first built a tree using the Classification and Regression Tree (CART) method.
```{r, message=FALSE, include = FALSE, warning=FALSE}
# Build model with default settings

set.seed(1982)
tree_tumor = rpart(Tumor~.,  #<- formula, response variable ~ predictors
                           #   "." means "use all other variables in data"
                            method = "class",#<- specify method, use "class" for tree
                            parms = list(split = "gini"),#<- method for choosing tree split
                            data = training2,#<- data used
                            control = rpart.control(cp=.01))

tree_tumor
# Most important variable is Converted.Stage

```

##### Decision Tree Plot
As we can see in the plot below, T4 is not used for training. This is unsurprising since there are only 6 individuals in the dataset with T4 tumors.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
# Plot the tree using the rpart.plot package (CART only).
rpart.plot(tree_tumor, type =4, extra = 101)

#The "cptable" element includes the optimal prunning based on the complexity parameter.
#View(tree_tumor$cptable)
```

##### CP Chart
The CP (complexity paramter) chart below helps us determine the optimal size of the tree. Based on our plot, the optimal size is 2.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
# Plot the cp chart and note the optimal size of the tree (CART only).

plotcp(tree_tumor) 

cptable_ex <- as_tibble(tree_tumor$cptable)

```

##### Variable Importance
Based on the model with default settings, the most important variable is AJCC.Stage, which describes the amount and spread of cancer in a patient's body.
```{r, message=FALSE, echo = FALSE, warning=FALSE}

tree_tumor$variable.importance

```

##### Prediction
We use the tree generated previously on the training dataset to predict the tumor classes with the test dataset.

```{r, message=FALSE, include = FALSE, warning=FALSE}

#Use the predict function and your models to predict the target variable using
#test set.

#tree_tumor$frame

tree_predict2 = predict(tree_tumor, test2,type= "class")

#tree_predict2 <- as.numeric(tree_predict2)

```

##### Evaluate Performance
Overall, the model doesn't do a great job at prediction. Based on the ROC curves below (starting with T1), the AUC is very low and does not indicate a very successful model.

T1 - The error rate for T1 is 100% and the detection rate is 0%. There are 2 T1 tumors in the test set and they are being incorrectly identified as T2 tumors.

T2 = The error rate for T2 is 27.27% and the detection rate is 42.86%. Compared to the base rate of 61.90%, the model does worse at predicting a T2 tumor.

T3 - The error rate for T3 is 33.33% and the detection rate is 14.29%. Compared to the base rate of 18.10%, this is not a huge improvement.

T4 - This type of tumor has a very low prevalence in the dataset; therefore, we are not surprised that 3 T4 tumors are being incorrectly identified as T2 and T3 tumors when we use the model.

```{r, message=FALSE, echo = FALSE, warning=FALSE}
# Generate, "by-hand", the hit rate and detection rate and compare the
#detection rate to your original baseline rate. How did your models work?

tum_conf_matrix = table(tree_predict2, test$Tumor)
tum_conf_matrix

#length(tree_predict2)
#length(test2$Tumor)

# The error rate is defined as a classification of "Parent" when
# this is not the case, and vice versa. It's the sum of all the
# values where a column contains the opposite value of the row.
#sum(tum_conf_matrix[tum_conf_matrix[,'T1'] != tum_conf_matrix[,'T1']])
# 59

#error_t1 = sum(tum_conf_matrix[c(2,3),1]/sum(tum_conf_matrix[,1])) * 100

# error_t1 = sum(tum_conf_matrix[c(2,3),1]/sum(tum_conf_matrix[,1])) * 100

#error_t2 = sum(tum_conf_matrix[c(1,3),2]/sum(tum_conf_matrix[,2])) * 100
#error_t3 = sum(tum_conf_matrix[c(2,1),3]/sum(tum_conf_matrix[,3])) * 100
#error_t4 = sum(tum_conf_matrix[c(1,2,3),4]/sum(tum_conf_matrix[,4])) * 100

# The error rate divides this figure by the total number of data points
# for which the forecast is created.
#sum(tum_conf_matrix)

# Let's use these values in 1 calculation.
#tum_error_rate = sum(tum_conf_matrix[row(tum_conf_matrix) != col(tum_conf_matrix)]) / sum(tum_conf_matrix)

#paste0("Hit Rate/True Error Rate:", tum_error_rate * 100, "%")

#Detection Rate is the rate at which the algo detects the positive class in proportion to the entire classification A/(A+B+C+D) where A is poss correctly predicted
#tum_conf_matrix

#det_t1 = tum_conf_matrix[1,1]/sum(tum_conf_matrix[,1]) * 100
#det_t2 = tum_conf_matrix[2,2]/sum(tum_conf_matrix[,2]) * 100
#det_t3 = tum_conf_matrix[3,3]/sum(tum_conf_matrix[,3]) * 100
#det_t4 = tum_conf_matrix[3,4]/sum(tum_conf_matrix[,4]) * 100

#table(cancer2$Tumor)

#We can adjust using a if else statement and the predicted prob
tree_example_prob2 = predict(tree_tumor, test2, type= "prob")


tree_example_prob2 = predict(tree_tumor, test2, type= "prob")

multiclass.roc(test2$Tumor, ifelse(tree_example_prob2[,'T1'] >= .05,0,1), plot=TRUE)
#multiclass.roc(test2$Tumor, ifelse(tree_example_prob2[,'T2'] >= .17 ,0,1), plot=TRUE)
#multiclass.roc(test2$Tumor, ifelse(tree_example_prob2[,'T3'] >= .49,0,1), plot=TRUE)
#multiclass.roc(test2$Tumor, ifelse(tree_example_prob2[,'T4'] >= .38,0,1), plot=TRUE)

```

#### Caret (C5.0 Style)

##### Building the Multi-Class C5.0 Model
Now, we will build a multi-class decision tree for tumor detection using the C5.0 algorithm.
```{r, message=FALSE, include = FALSE, warning=FALSE}
#Cross validation process
set.seed(1982)
cancer_train_rows2 = sample(1:nrow(cancer2),
                              round(0.8 * nrow(cancer2), 0),
                              replace = FALSE)
# Check to make sure we have 80% of the rows

training2 = cancer2[cancer_train_rows2, ]
# Rows not used in training set, aka the test set
test2 = cancer2[-cancer_train_rows2, ]

```


```{r, message=FALSE, include = FALSE, warning=FALSE}

fitControl <- trainControl(method = "repeatedcv",
  number = 10,
  repeats = 5, returnResamp="all") #setting up our cross validation

# number - number of folds
# repeats - number of times the cv is repeated, here it's 5 take the average of
# those 5 repeats

# Choose the features and classes

#View(training2)

training2 <- subset(training2, select=-c(Gender, HER2.Final.Status, Node.Coded, Metastasis, Metastasis.Coded, AJCC.Stage, Converted.Stage, Survival.Data.Form, Vital.Status))
test2 <- subset(test2, select=-c(Gender, HER2.Final.Status, Node.Coded, Metastasis, Metastasis.Coded, AJCC.Stage, Converted.Stage, Survival.Data.Form, Vital.Status))

#features <- subset(training2, select=-c(Tumor, Gender, ER.Status))
features <- subset(training2, select=-c(Tumor))
target <- as.factor(training2$Tumor)

# str(features)
# table(features$Vital.Status)
# str(target)

features <- as.data.frame(features)

grid <- expand.grid(.winnow = c(TRUE,FALSE), .trials=c(1,5,10,15,20), .model="tree" )

#expand.grid - function in caret that will essentially conduct a hyper-parameter
# and select the best options

#winnow - whether to reduce the feature space - uses a regulator/penalty
#trails - number of boosting iterations to try, 1 indicates a single model
#model - type of ml model


#tumor_mdl <- C5.0(features, target, tuneGrid=grid, trControl=fitControl, verbose=TRUE)
set.seed(1982)
tumor_mdl <- train(x=features,y=target,tuneGrid=grid,trControl=fitControl,method="C5.0"
            ,verbose=TRUE)

```


```{r, message=FALSE, echo = FALSE, warning=FALSE}
tumor_mdl

#View(tumor_mdl$pred)

```

##### Creating Tree
The XY plot below visualizes the accuracy for different numbers of trials. It is clear that the number of trials does not have a significant impact on the accuracy. When creating the tree, we encountered issues with predicting the model on the test data. If we use all columns in the dataset for prediction, we can create a tree with up to 83% accuracy. However, due to undefined cases in the data, we are unable to use this model for prediction. Therefore, we removed 10 columns in order to create a tree that is useful for prediction. 

```{r, message=FALSE, echo = FALSE, warning=FALSE}
# visualize the re-sample distributions
xyplot(tumor_mdl,type = c("g", "p", "smooth"))

# varImp(tumor_mdl)

```

##### Evaluate Performance
Due to the complications with prediction, we can only evaluate our tree using 5 columns. As evident in our confusion matrix, the model does not perform very well. Although all the T2 tumors were correctly predicted, all the other tumors were incorrectly classified as a T2 tumor (which is evident in the Specificity values of 1 for T1, T3 and T4 tumors). The accuracy is 66.67%, which isn't terrible, but knowing that only the T2 tumors are being classified correctly, we are not impressed with this number. We have a Kappa value of 0, which is very low. Overall, the C5.0 model we generated to predict the tumor class is not successful, and is much worse in comparison to the CART model. 

```{r, message=FALSE, echo = FALSE, warning=FALSE}

#test2$Tumor <- as.numeric(test2$Tumor)
tumor_predict = predict(tumor_mdl, test2, type= "raw")
# View(as.data.frame(tumor_predict))

#View(as_tibble(wine_predict))


#Lets use the confusion matrix

set.seed(1982)
confusionMatrix(as.factor(tumor_predict), as.factor(test2$Tumor), 
                 dnn=c("Prediction", "Actual"), mode = "sens_spec")

#table(test2$Tumor)


tumor_predict_p = predict(tumor_mdl,test2, type= "prob")

```

## Conclusion
Overall, this lab was challenging and the results of our models were unsuccessful. 

### PR.Status
We performed the CART and C5.0 analysis on PR.Status first (a binary variable). We expected the CART model to be more successful, which it was. However, both CART and C5.0 models were pretty bad and need to be improved. There are several ways to improve the model, but we would first suggest gathering more data, specifically more consistent and balanced data. This will make the models much more accurate for prediction. Moving forward, the models we built could be helpful in predicting whether or not someone will be positive for PR.Status, but should definitely not be used for practice. We would need to significantly improve the models before using them in practice, especially since the stakes are so high if we are using this model to detect the presence of cancer.

### Tumor
We used the same two models (CART and C5.0) to build trees for our multi-class variable, tumor. There were four classes of tumors with varying prevalence in the dataset. We expected the C5.0 analysis to be better, since that algorithm is better suited for multi-class prediction. However, the CART model proved to be a bit more successful, although not good by any means. The T4 tumor had a very low prevalence in the data (only 6 indnividuals with this tumor) so it was not used for training the model. The CART model did an okay job at identifying T1, T2, and T3 variables, but could definitely be improved. The C5.0 model was surprisingly bad; due to undefined cases in the data we had to remove 10 columns. Therefore, we were only training the model on 5 variables. The model resulted in identifying all tumors as T2 tumors... definitely not the result we were hoping for. As previously mentioned, the model could likely be improved if we had more data and more balanced data that had near equal prevalence for all tumors. We definitely would not want to use this model in practice unless it is significantly improved. If we are able to improve the model, it could be useful in predicting whether or not a patient as a malignant/cancerous tumor.