---
title: "Decision Tree Lab"
author: "Amanda Rein, Belen Gomez Grimaldi, Kay Mattern"
date: "4/28/2021"
output:
  html_document:
    toc: TRUE
    theme: journal
    toc_float: TRUE
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r knitr_options, include=FALSE}
set.seed(19)
```


```{r load-packages, include=FALSE}
#install.packages("rio")
library(rio)
#install.packages("plyr")
library(plyr)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("rpart")
library(rpart)
#install.packages("psych")
library(psych)
#install.packages("pROC")
library(pROC)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("rattle")
library(rattle)
# setwd("/cloud/project/decision_trees")
#install.packages("caret")
install.packages("caret")
library(caret)
#install.packages("C50")
#install.packages("mlbench")
#install.packages("naniar")
library(naniar)
#install.packages("e1071")
library(e1071)
```

# Cleaning the Data
In order to prepare the data for decision tree training, we first removed ER.Status because it contains essentially the same information as PR.Status. Next, we removed Days.to.date.of.Death because, as the missing variable plot shows, this variable was missing for a lot of patients. Additionally, we dropped any cases after that that were not complete. 

We thought that this would be enough, but we soon learned that there were some columns that had an imbalance of information, causing our models to not build correctly. We had two solutions for this problem. First, we dropped the Gender, Metastasis, and Metastasis.Coded columns because they did not provide very much information because only a couple of patients had different values. Also, we collapsed the Stage columns (AJCC.Stage and Converted.Stage) so that the information in those columns was more balanced while still maintaining the factors. To do this, we put, for example, all Stage I, Stage IA, Stage IB variables into one category. Finally, we dropped any patients that contained unique values in the columns because if they ended up in the test set, then we would run into issues introducing new values into the decision tree.

After that, we were ready to start creating our trees! We broke down the remaining 102 patients into test and training sets.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
#1 Load the data and ensure the column names don't have spaces, hint check.names.  
cancer <- tibble(import("clinical_breast_cleaned.csv", check.names= TRUE))
gg_miss_var(cancer)
cancer <- select(cancer, -c(ER.Status, Days.to.date.of.Death)) # ER.Status same as PR.Status, Days.to.date.of.Death has lots of missing values, rest has new levels in predict
# gg_miss_var(cancer)
cancer <- cancer[complete.cases(cancer), ]

cancer$AJCC.Stage <- fct_collapse(cancer$AJCC.Stage,
                               I = c("Stage I", "Stage IA", "Stage IB"),
                               II =c("Stage II", "Stage IIA", "Stage IIB"),
                               III = c("Stage III", "Stage IIIA", "Stage IIIB", "Stage IIIC"),
                               IV = c("Stage IV"))
cancer$Converted.Stage <- fct_collapse(cancer$Converted.Stage,
                               None = c("No_Conversion"),
                               I =c("Stage I"),
                               II = c("Stage IIA", "Stage IIB"),
                               III = c("Stage IIIA", "Stage IIIB", "Stage IIIC"))


cancer <- cancer[-c(1, 7,8)]
cancer <- cancer[cancer$HER2.Final.Status != "Equivocal", ]
cancer <- cancer[cancer$AJCC.Stage != "IV", ]

cancer$HER2.Final.Status <- as.factor(cancer$HER2.Final.Status)
cancer$PR.Status <- as.factor(cancer$PR.Status)


set.seed(1982)
cancer_train_rows = sample(1:nrow(cancer),
                              round(0.8 * nrow(cancer), 0),
                              replace = FALSE)
# Check to make sure we have 80% of the rows

training = cancer[cancer_train_rows, ]
# Rows not used in training set, aka the test set
test = cancer[-cancer_train_rows, ]
```

## Analysis for PR.Status and Tumor {.tabset}

After cleaning our data, we began our analysis on both the PR.Status variable and the Tumor variable. Since PR.Status is a binary variable, we chose to run a CART-style analysis to create a decision tree. On the other hand, Tumor is a multi-class variable, so a C5.0 analysis makes more sense. 

### PR.Status {.tabset}

#### RPart (CART style)
CART analysis tends to be more useful for binary predictions, so we think that this type of tree model will work best for the PR.Status variable.

##### Base Rate
The base rate is 51.9% for identifying whether PR.Status is positive or not. 
```{r, message=FALSE, echo = FALSE, warning=FALSE}
# (x <- 1- sum(cancer$PR.Status)/length(cancer$PR.Status)) # 48.57% have biomarker for PR.Status

split <- table(cancer$PR.Status)[2] / sum(table(cancer$PR.Status))
# split
```

##### Building the Model
To build the model, we used the RPART to create a tree. From this tree, we were able to identify the most relevant variables. In this case, OS.Time was the most relevant variable. We know this because it is at the top of the tree, as you can see below, which means that it is the variable that most definitively partitions the data set to identify positive or negative PR.Status values. The relative importance of each variable can be seen in the table below.

Additionally, we looked at the conditional probability table. From that, we gathered that we probably was to have a tree with 2 splits to reduce the xerror (cross-validated error) as much as possible. This can be seen in the graph below, where 3 is the leftmost value below the dotted line (representing the highest cross-validated error minus the minimum cross-validated error, plus the standard deviation of the error at that tree).
```{r, message=FALSE, echo = FALSE, warning=FALSE}
#7 Build your model using the default settings
set.seed(1981)
tree_prstatus = rpart(PR.Status~.,  #<- formula, response variable ~ predictors
                           #   "." means "use all other variables in data"
                            method = "class",#<- specify method, use "class" for tree
                            parms = list(split = "gini"),#<- method for choosing tree split
                            data = training,#<- data used
                            control = rpart.control(cp=.01))

```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#8 View the results, what is the most important variable for the tree? 

# tree_prstatus # root, Days.to.date.of.Last.Contact
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#9 Plot the tree using the rpart.plot package (CART only).
rpart.plot(tree_prstatus, type =4, extra = 101)#package rpart.plot
#export this to  pdf for better viewing

#The "cptable" element includes the optimal prunning based on the complexity parameter.
#View(tree_prstatus$cptable)
#The "cptable" element includes the optimal pruning based on the complexity parameter.
cp <- tree_prstatus$cptable


# table(training[,2])
# table(test[,2])
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#10 plot the cp chart and note the optimal size of the tree (CART only).

most_imp_vars <- tree_prstatus$variable.importance
most_imp_vars




plotcp(tree_prstatus) 

cptable_ex <- as_tibble(tree_prstatus$cptable)
```

##### Prediction and Error Analysis

In this section, we used the tree that we created to predict the target variable, PR.Status, for each patient in the test set.

After running the model on our test set, we then generated the confusion matrix to compare the results to the actual data. As it can be seen, we ran into the issue of overclassifying as negative. As such, our detection rate was incredibly low, at only 15.8%. Our error rate was 10 out of 19 test cases, which is a terrible rate. This is below the baseline, unfortunately. The error rate was 52.6%, which is not ideal. This can be seen even more through our ROC plot that only has 48.3% of the area under the curve, which demonstrates that we are not functioning above random guessing. The only value of our evaluation metrics that worked okay was the specificity of the model, which is sitting at 66.7%, showing that we were correctly identifying negative results at a decent level.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
#11 Use the predict function and your tree to predict the target variable using test set. 
# tree_prstatus$frame

tree_predict = predict(tree_prstatus,test, type= "class")

#View(as.data.frame(tree_predict))

tree_predict <- as.numeric(tree_predict)
#View(tree_predict)
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#12 Generate, "by-hand", the hit rate and detection rate and compare the 
#detection rate to your original baseline rate. How did your models work?

tree_predict <- ifelse(tree_predict >= 2,1,0)

install.packages("e1071")
library(e1071)
par_conf_matrix <- confusionMatrix(as.factor(tree_predict), as.factor(test$PR.Status), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

conf_matrix <- par_conf_matrix$table
conf_matrix

# The error rate is defined as a classification of "Parent" when 
# this is not the case, and vice versa. It's the sum of all the
# values where a column contains the opposite value of the row.
error_rate = sum(conf_matrix[row(conf_matrix)!= col(conf_matrix)])
# 10


# The error rate divides this figure by the total number of data points
# for which the forecast is created.
# sum(conf_matrix)

# Let's use these values in 1 calculation.
par_error_rate = sum(conf_matrix[row(conf_matrix) != col(conf_matrix)]) / sum(conf_matrix)

# paste0("Hit Rate/True Error Rate:", par_error_rate * 100, "%")

#Detection Rate is the rate at which the algo detects the positive class in proportion to the entire classification A/(A+B+C+D) where A is poss correctly predicted
#par_conf_matrix
# conf_matrix

# conf_matrix[2,2]/sum(conf_matrix)

# table(cancer$PR.Status)

#We can adjust using a if else statement and the predicted prob

tree_example_prob = predict(tree_prstatus,test, type= "prob")
#View(tree_example_prob)

#Let's 

# tree_example_prob[,"0"]
roc(test$PR.Status, ifelse(tree_example_prob[,"0"] >= .50,0,1), plot=TRUE)
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#13 Use the the confusion matrix function in caret to 
#check a variety of metrics and comment on the metric that might be best for 
#each type of analysis.  

#confusionMatrix(as.factor(tree_predict), as.factor(test$PR.Status), positive = "PR.Status", dnn=c("Prediction", "Actual"), mode = "sens_spec")
con_PR_RPART = confusionMatrix(as.factor(tree_predict), as.factor(test$PR.Status), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#14 Generate a ROC and AUC output, interpret the results

# par_roc <- roc(test$PR.Status, as.numeric(tree_predict), plot = TRUE) #Building the evaluation ROC and AUV using the predicted and original target variables 

# par_roc

# plot(par_roc)
```

#### Caret (C5.0 style)

Caret analyses tend to be better suited for multi-factor or continuous classifications, so we assume that this type of modelling will be worst for identifying the PR.Status variable.

##### Creating Tree

Using the cross validation process and tuning, we found the best combination of attributes for our tree. This ended up being only one trial and no winnowing. However, most models seemed to have a very similar accuracy, as can be seen on the accuracy versus trial plots below.

We ended up having to remove more columns from our data because they were causing trouble in the algorithm due to undefined cases in the test set. Because of this, we were down to only four variables. However, it was found that three of these variables were important in deciding whether PR.Status was positive or negative. Of these, OS.event was found to be the most important. The relative importance of each variable can be seen in the table below.

```{r, message=FALSE, echo = FALSE, warning=FALSE}
install.packages("C50")
library(C50) #Need this to pass into caret 
install.packages("mlbench")
library(mlbench)

#Cross validation process 

fitControl <- trainControl(method = "repeatedcv",
  number = 10,
  repeats = 5, returnResamp="all") #setting up our cross validation

# number - number of folds
# repeats - number of times the cv is repeated, here it's 5 take the average of
# those 5 repeats

# Choose the features and classes

# View(training)

training$PR.Status <- as.factor(training$PR.Status)
test$PR.Status <- as.factor(test$PR.Status)

grid <- expand.grid(.winnow = c(TRUE,FALSE), .trials=c(1,5,10,15,20), .model="tree" )

# grid <- as.data.frame(grid)

#expand.grid - function in caret that will essentially conduct a hyper-parameter 
# and select the best options
# expand.grid(grid)

#winnow - whether to reduce the feature space - uses a regulator/penalty
#trails - number of boosting iterations to try, 1 indicates a single model 
#model - type of ml model

features <- as.data.frame(training[,c(-2)])
features <- features[, -c(2,3,4,5,6,7,8)]

target <- as.factor(training$PR.Status)

options(warn = -1)
PR_mdl <- train(x=features,y=target,tuneGrid=grid,trControl=fitControl,method="C5.0",verbose=TRUE)

PR_mdl

# str(PR_mdl$pred)

# visualize the re-sample distributions
xyplot(PR_mdl,type = c("g", "p", "smooth"))

varImp(PR_mdl)
```

##### Analyzing Results

As it can be seen in the confusion matrix below, our results are beginning to look promising. We never had any false negatives, which is a good thing. This is reflected in our perfect Specificity score. Our accuracy was 78.9%, which is significantly higher than that of our base rate. Further, our kappy value is 0.57, which is an okay value for that metric. Our sensitivity is the main place where we could use some work, as it is only at 55.6%. 

The overall error rate was 21.0%, which is not ideal but certainly a step in the right direction for this model. The area under the curve of our POC plot is 77.8%, which is also a great improvement over random guessing.

```{r, message=FALSE, echo = FALSE, warning=FALSE}
PRC50_predict = predict(PR_mdl,test, type="raw")

#Lets use the confusion matrix

conf_matrix_PRC50 <- confusionMatrix(as.factor(PRC50_predict), as.factor(test$PR.Status), 
                dnn=c("Prediction", "Actual"), mode = "sens_spec")


conf_matrix_PRC50
# table(test$PR.Status)


PRC50_predict_p = predict(PR_mdl,test, type= "prob")



cm_PRC50 <- conf_matrix_PRC50$table
cm_PRC50

# The error rate is defined as a classification of "Parent" when 
# this is not the case, and vice versa. It's the sum of all the
# values where a column contains the opposite value of the row.
error_rate = sum(cm_PRC50[row(cm_PRC50)!= col(cm_PRC50)])

# 4


# The error rate divides this figure by the total number of data points
# for which the forecast is created.
# sum(conf_matrix)

# Let's use these values in 1 calculation.
PRC50_error_rate = sum(cm_PRC50[row(cm_PRC50) != col(cm_PRC50)]) / sum(cm_PRC50)
# PRC50_error_rate


PR50ROC <- roc(test$PR.Status, as.numeric(PRC50_predict), plot = TRUE)

PR50ROC



# PR_predict_p

```



#### **** {.tabset}
### Multi-Class Prediction: Tumors {.tabset}

Next, we will build another decision tree to predict the type of tumor a patient has. There are four classes of tumors (T1, T2, T3, T4).
#### RPART (CART style)
```{r, message=FALSE, echo = FALSE, warning=FALSE}
#15 Follow the same steps for the multi-class target, tumor, aside from step 1,
# 2 and 14. For step 13 compare to the four base rates and see how you did.
cancer2 <- tibble(import("clinical_breast_cleaned.csv", check.names= TRUE))
cancer2 <- select(cancer2, -c(Days.to.date.of.Death)) #Days.to.date.of.Death has lots of missing values, rest has new levels in predict
cancer2 <- cancer2[complete.cases(cancer2), ]
describe(cancer2)
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
# Split into train/test

split <- createDataPartition(cancer2$Tumor,times=1,p = 0.8,list=FALSE)
training2 <- cancer2[split,]
test2 <- cancer2[-split,]

```

##### Base Rates

The base rates for the four tumor classes are:

- T1 = 14.29%

- T2 = 61.90%

- T3 = 18.10%

- T4 = 5.71%

```{r, message=FALSE, echo = FALSE, warning=FALSE}
# Calculate Baserates
table(cancer2$Tumor)

t1_base <- sum(cancer2$Tumor == "T1") / length(cancer2$Tumor) # 14.29%
t2_base <- sum(cancer2$Tumor == "T2") / length(cancer2$Tumor) # 61.90%
t3_base <- sum(cancer2$Tumor == "T3") / length(cancer2$Tumor) # 18.10%
t4_base <- sum(cancer2$Tumor == "T4") / length(cancer2$Tumor) # 5.71%

```

##### Building the Multi-Class CART Model
```{r, message=FALSE, echo = FALSE, warning=FALSE}
# Build model with default settings

set.seed(1981)
tree_tumor = rpart(Tumor~.,  #<- formula, response variable ~ predictors
                           #   "." means "use all other variables in data"
                            method = "class",#<- specify method, use "class" for tree
                            parms = list(split = "gini"),#<- method for choosing tree split
                            data = training2,#<- data used
                            control = rpart.control(cp=.01))

```

Based on the model with default settings, the most important variable is AJCC.Stage, which describes the amount and spread of cancer in a patient's body.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
# View results, most important variable

tree_tumor

# Most important variable is AJCC.Stage

```

##### Decision Tree Plot
```{r, message=FALSE, echo = FALSE, warning=FALSE}
# Plot the tree using the rpart.plot package (CART only).
rpart.plot(tree_tumor, type =4, extra = 101)

#The "cptable" element includes the optimal prunning based on the complexity parameter.
#View(tree_tumor$cptable)
```

##### CP Chart
The CP (complexity paramter) chart below helps us determine the optimal size of the tree. Based on our plot, the optimal size is 4.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
# Plot the cp chart and note the optimal size of the tree (CART only).

plotcp(tree_tumor) #Produces a "elbow chart" for various cp values, it's actually a terrible chart, but somewhat useful, dashed line represents the highest cross-validated error minus the minimum cross-validated error, plus the standard deviation of the error at that tree. A reasonable choice of cp for pruning is often the leftmost value where the mean is less than the horizontal line. Not the difference between the size of tree and the nsplits. Size is the number of terminal nodes.

cptable_ex <- as_tibble(tree_tumor$cptable)
cptable_ex

#Shows the reduction in error provided by include the variable
tree_tumor$variable.importance

```

##### Prediction
We use the tree generated previously on the training dataset to predict the tumor classes with the test dataset.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
#Use the predict function and your models to predict the target variable using
#test set.

tree_tumor$frame

tree_predict2 = predict(tree_tumor, test2, type= "class")

#View(as.data.frame(tree_predict2))

tree_predict2 <- as.numeric(tree_predict2)
#View(tree_predict)
```

##### Evaluate Performance
```{r, message=FALSE, echo = FALSE, warning=FALSE}
# Generate, "by-hand", the hit rate and detection rate and compare the
#detection rate to your original baseline rate. How did your models work?

tum_conf_matrix = table(tree_predict2, test2$Tumor)

# The error rate is defined as a classification of "Parent" when
# this is not the case, and vice versa. It's the sum of all the
# values where a column contains the opposite value of the row.
#sum(tum_conf_matrix[tum_conf_matrix[,'T1'] != tum_conf_matrix[,'T1']])
# 59

error_t1 = sum(tum_conf_matrix[c(2,3),1]/sum(tum_conf_matrix[,1])) * 100
error_t2 = sum(tum_conf_matrix[c(1,3),2]/sum(tum_conf_matrix[,2])) * 100
error_t3 = sum(tum_conf_matrix[c(2,1),3]/sum(tum_conf_matrix[,3])) * 100
error_t4 = sum(tum_conf_matrix[c(1,2,3),4]/sum(tum_conf_matrix[,4])) * 100

# The error rate divides this figure by the total number of data points
# for which the forecast is created.
#sum(tum_conf_matrix)

# Let's use these values in 1 calculation.
#tum_error_rate = sum(tum_conf_matrix[row(tum_conf_matrix) != col(tum_conf_matrix)]) / sum(tum_conf_matrix)

paste0("Hit Rate/True Error Rate:", tum_error_rate * 100, "%")

#Detection Rate is the rate at which the algo detects the positive class in proportion to the entire classification A/(A+B+C+D) where A is poss correctly predicted
tum_conf_matrix

det_t1 = tum_conf_matrix[1,1]/sum(tum_conf_matrix[,1]) * 100
det_t2 = tum_conf_matrix[2,2]/sum(tum_conf_matrix[,2]) * 100
det_t3 = tum_conf_matrix[3,3]/sum(tum_conf_matrix[,3]) * 100
det_t4 = tum_conf_matrix[3,4]/sum(tum_conf_matrix[,4]) * 100

#table(cancer2$Tumor)

#We can adjust using a if else statement and the predicted prob

tree_example_prob2 = predict(tree_tumor, test2, type= "prob")

multiclass.roc(test2$Tumor, ifelse(tree_example_prob2[,'T1'] >= .05,0,1), plot=TRUE)
multiclass.roc(test2$Tumor, ifelse(tree_example_prob2[,'T2'] >= .17 ,0,1), plot=TRUE)
multiclass.roc(test2$Tumor, ifelse(tree_example_prob2[,'T3'] >= .49,0,1), plot=TRUE)
multiclass.roc(test2$Tumor, ifelse(tree_example_prob2[,'T4'] >= .38,0,1), plot=TRUE)

```
T1 - The error rate for T1 is 33.33% and the detection rate is 66.67%. Compared to the base rate of 14.29%, the model does a decent job at predicting T1 tumors. In terms of the ROC curve, a threshold of 0.05 maximizes the AUC at 0.6816.

T2 = The error rate for T2 is 23.08% and the detection rate is 76.92%. Compared to the base rate of 61.90%, the model improves the accuracy of predicting a T2 tumor. The ROC curve with a threshold of 0.17 maximizes the AUC at 0.6752.

T3 - The error rate for T3 is 33.33% and the detection rate is 66.67%. Compared to the base rate of 18.10%, the model significantly improves the accuracy of predicting a T3 tumor. In terms of the ROC curve, a threshold of 0.49 gives us an ROC that appears to have a slope of 1, but the AUC is 0.7585. Therefore, the ROC curve for T3 tumors is not entirely accurate and we should be cautious with information provided by this curve.

T4 - The error rate for T4 is 100% and the detection rate is 0%. This means that the test set had only 1 individual with a T4 tumor, and that tumor was not correctly predicted. Since there are only 6 individuals with T4 tumors in the original dataset, this is not too shocking. Although the ROC curve is not particularly useful in this situation, we achieve an AUC of 0.485 when the threshold is set to 0.38.


#### Building the Multi-Class C5.0 Model
Now, we will build a multi-class decision tree for tumor detection using the C5.0 algorithm.
```{r}
#Cross validation process

fitControl <- trainControl(method = "repeatedcv",
  number = 10,
  repeats = 5, returnResamp="all") #setting up our cross validation

# number - number of folds
# repeats - number of times the cv is repeated, here it's 5 take the average of
# those 5 repeats

# Choose the features and classes

#View(training2)

features <- training2[,c(-6)]
target <- training2$Tumor

str(features)
str(target)

grid <- expand.grid(.winnow = c(TRUE,FALSE), .trials=c(1,5,10,15,20), .model="tree" )

#expand.grid - function in caret that will essentially conduct a hyper-parameter
# and select the best options

#winnow - whether to reduce the feature space - uses a regulator/penalty
#trails - number of boosting iterations to try, 1 indicates a single model
#model - type of ml model

tumor_mdl <- train(x=features,y=target,tuneGrid=grid,trControl=fitControl,method="C5.0"
            ,verbose=TRUE)

tumor_mdl

#View(tumor_mdl$pred)

# visualize the re-sample distributions
xyplot(tumor_mdl,type = c("g", "p", "smooth"))

varImp(tumor_mdl)



```

##### Evaluate Performance
```{r}
wine_predict = predict(wine_mdl,test_w, type= "raw")

#View(as_tibble(wine_predict))


#Lets use the confusion matrix

confusionMatrix(as.factor(wine_predict), as.factor(test_w$text_rank),
                dnn=c("Prediction", "Actual"), mode = "sens_spec")

table(test_w$text_rank)


wine_predict_p = predict(wine_mdl,test_w, type= "prob")



```



```{r, message=FALSE, echo = FALSE, warning=FALSE}
# 16 Summarize what you learned for each model along the way and make
# recommendations to the world on how this could be used moving forward,
# being careful not to over promise.
```
