---
title: "Decision Tree Lab"
author: "Amanda Rein, Belen Gomez Grimaldi, Kay Mattern"
date: "4/28/2021"
output:
  html_document:
    toc: TRUE
    theme: journal
    toc_float: TRUE
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-packages, include=FALSE}
install.packages("rio")
library(rio)
install.packages("plyr")
library(plyr)
install.packages("tidyverse")
library(tidyverse)
install.packages("rpart")
library(rpart)
install.packages("psych")
library(psych)
install.packages("pROC")
library(pROC)
install.packages("rpart.plot")
library(rpart.plot)
install.packages("rattle")
library(rattle)
install.packages("caret")
library(caret)
install.packages("C50")
install.packages("mlbench")
install.packages("naniar")
library(naniar)
```

# Cleaning the Data
In order to prepare the data for decision tree training, we first removed ER.Status because it contains essentially the same information as PR.Status. Next, we removed Days.to.date.of.Death because, as the missing variable plot shows, this variable was missing for a lot of patients. Additionally, we dropped any cases after that that were not complete. 

We thought that this would be enough, but we soon learned that there were some columns that had an imbalance of information, causing our models to not build correctly. We had two solutions for this problem. First, we dropped the Gender, Metastasis, and Metastasis.Coded columns because they did not provide very much information because only a couple of patients had different values. Also, we collapsed the Stage columns (AJCC.Stage and Converted.Stage) so that the information in those columns was more balanced while still maintaining the factors. To do this, we put, for example, all Stage I, Stage IA, Stage IB variables into one category. Finally, we dropped any patients that contained unique values in the columns because if they ended up in the test set, then we would run into issues introducing new values into the decision tree.

After that, we were ready to start creating our trees! We broke down the remaining 102 patients into test and training sets.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
#1 Load the data and ensure the column names don't have spaces, hint check.names.  
cancer <- tibble(import("clinical_breast_cleaned.csv", check.names= TRUE))
gg_miss_var(cancer)
cancer <- select(cancer, -c(ER.Status, Days.to.date.of.Death)) # ER.Status same as PR.Status, Days.to.date.of.Death has lots of missing values, rest has new levels in predict
# gg_miss_var(cancer)
cancer <- cancer[complete.cases(cancer), ]
describe(cancer)

cancer$AJCC.Stage <- fct_collapse(cancer$AJCC.Stage,
                               I = c("Stage I", "Stage IA", "Stage IB"),
                               II =c("Stage II", "Stage IIA", "Stage IIB"),
                               III = c("Stage III", "Stage IIIA", "Stage IIIB", "Stage IIIC"),
                               IV = c("Stage IV"))
cancer$Converted.Stage <- fct_collapse(cancer$Converted.Stage,
                               None = c("No_Conversion"),
                               I =c("Stage I"),
                               II = c("Stage IIA", "Stage IIB"),
                               III = c("Stage IIIA", "Stage IIIB", "Stage IIIC"))


cancer <- cancer[-c(1, 7,8)]
cancer <- cancer[cancer$HER2.Final.Status != "Equivocal", ]
cancer <- cancer[cancer$AJCC.Stage != "IV", ]

cancer$HER2.Final.Status <- as.factor(cancer$HER2.Final.Status)


x <- createDataPartition(cancer$PR.Status,times=1,p = 0.8,list=FALSE)
training <- cancer[x,]
test <- cancer[-x,]

```

## Analysis for PR.Status and Tumor {.tabset}

After cleaning our data, we began our analysis on both the PR.Status variable and the Tumor variable. Since PR.Status is a binary variable, we chose to run a CART-style analysis to create a decision tree. On the other hand, Tumor is a multi-class variable, so a C5.0 analysis makes more sense. 

### PR.Status - RPart (CART style)

#### Base Rate
The base rate is 51.4% for identifying whether PR.Status is positive or not. 
```{r, message=FALSE, echo = FALSE, warning=FALSE}
(x <- 1- sum(cancer$PR.Status)/length(cancer$PR.Status)) # 48.57% have biomarker for PR.Status

split <- table(cancer$PR.Status)[2] / sum(table(cancer$PR.Status))
split
```

#### Building the Model
To build the model, we used the R
```{r, message=FALSE, echo = FALSE, warning=FALSE}
#7 Build your model using the default settings
set.seed(1981)
tree_prstatus = rpart(PR.Status~.,  #<- formula, response variable ~ predictors
                           #   "." means "use all other variables in data"
                            method = "class",#<- specify method, use "class" for tree
                            parms = list(split = "gini"),#<- method for choosing tree split
                            data = training,#<- data used
                            control = rpart.control(cp=.01))

```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#8 View the results, what is the most important variable for the tree? 

tree_prstatus # root, Days.to.date.of.Last.Contact
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#9 Plot the tree using the rpart.plot package (CART only).
rpart.plot(tree_prstatus, type =4, extra = 101)#package rpart.plot
#export this to  pdf for better viewing

#The "cptable" element includes the optimal pruning based on the complexity parameter.
View(tree_prstatus$cptable)

```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#10 plot the cp chart and note the optimal size of the tree (CART only).

plotcp(tree_prstatus) #Produces a "elbow chart" for various cp values, it's actually a terrible chart, but somewhat useful, dashed line represents the highest cross-validated error minus the minimum cross-validated error, plus the standard deviation of the error at that tree. A reasonable choice of cp for pruning is often the leftmost value where the mean is less than the horizontal line. Not the difference between the size of tree and the nsplits. Size is the number of terminal nodes. 

cptable_ex <- as_tibble(tree_prstatus$cptable)
cptable_ex

#Shows the reduction in error provided by include the variable 
tree_prstatus$variable.importance
```


```{r, message=FALSE, echo = FALSE, warning=FALSE}
#11 Use the predict function and your models to predict the target variable using
#test set. 
tree_prstatus$frame

tree_predict = predict(tree_prstatus,test, type= "class")

View(as.data.frame(tree_predict))

#tree_predict <- as.numeric(tree_predict)
View(tree_predict)
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#12 Generate, "by-hand", the hit rate and detection rate and compare the 
#detection rate to your original baseline rate. How did your models work?
install.packages("e1071")
library(e1071)
par_conf_matrix <- confusionMatrix(as.factor(tree_predict), as.factor(test$PR.Status), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")

conf_matrix <- par_conf_matrix$table

# The error rate is defined as a classification of "Parent" when 
# this is not the case, and vice versa. It's the sum of all the
# values where a column contains the opposite value of the row.
sum(conf_matrix[row(conf_matrix)!= col(conf_matrix)])
# 10


# The error rate divides this figure by the total number of data points
# for which the forecast is created.
sum(conf_matrix)

# Let's use these values in 1 calculation.
par_error_rate = sum(conf_matrix[row(conf_matrix) != col(conf_matrix)]) / sum(conf_matrix)

paste0("Hit Rate/True Error Rate:", par_error_rate * 100, "%")

#Detection Rate is the rate at which the algo detects the positive class in proportion to the entire classification A/(A+B+C+D) where A is poss correctly predicted
conf_matrix

conf_matrix[2,2]/sum(conf_matrix)

table(cancer$PR.Status)

#We can adjust using a if else statement and the predicted prob

tree_example_prob = predict(tree_prstatus,test, type= "prob")
View(tree_example_prob)

#Let's 

tree_example_prob[,"0"]
roc(test$PR.Status, ifelse(tree_example_prob[,"0"] >= .50,0,1), plot=TRUE)

```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#13 Use the the confusion matrix function in caret to 
#check a variety of metrics and comment on the metric that might be best for 
#each type of analysis.  

confusionMatrix(as.factor(tree_predict), as.factor(test$PR.Status), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#14 Generate a ROC and AUC output, interpret the results

par_roc <- roc(test$PR.Status, as.numeric(tree_predict), plot = TRUE) #Building the evaluation ROC and AUV using the predicted and original target variables 

par_roc

plot(par_roc)
```

```{r, message=FALSE, echo = FALSE, warning=FALSE}
#15 Follow the same steps for the multi-class target, tumor, aside from step 1, 
# 2 and 14. For step 13 compare to the four base rates and see how you did. 
```

### PR.Status - Caret (C5.0 style)

```{r, message=FALSE, echo = FALSE, warning=FALSE}
install.packages("C50")
library(C50) #Need this to pass into caret 
install.packages("mlbench")
library(mlbench)

#Cross validation process 

fitControl <- trainControl(method = "repeatedcv",
  number = 10,
  repeats = 5, returnResamp="all") #setting up our cross validation

# number - number of folds
# repeats - number of times the cv is repeated, here it's 5 take the average of
# those 5 repeats

# Choose the features and classes

View(training)

training$PR.Status <- as.factor(training$PR.Status)
test$PR.Status <- as.factor(test$PR.Status)


features <- as.data.frame(training[,c(-2)])
target <- as.factor(training$PR.Status)

str(features)
str(target)

typeof(features)
typeof(grid)
typeof(target)
typeof(fitControl)

grid <- expand.grid(.winnow = c(TRUE,FALSE), .trials=c(1,5,10,15,20), .model="tree" )

# grid <- as.data.frame(grid)

#expand.grid - function in caret that will essentially conduct a hyper-parameter 
# and select the best options
# expand.grid(grid)

#winnow - whether to reduce the feature space - uses a regulator/penalty
#trails - number of boosting iterations to try, 1 indicates a single model 
#model - type of ml model
typeof(grid)

PR_mdl <- train(x=features,y=target,tuneGrid=grid,trControl=fitControl,method="C5.0",verbose=TRUE)

PR_mdl

View(PR_mdl$pred)

# visualize the re-sample distributions
xyplot(PR_mdl,type = c("g", "p", "smooth"))

varImp(PR_mdl)



```

Let's use the model to predict and the evaluate the performance
```{r, message=FALSE, echo = FALSE, warning=FALSE}
PR_predict = predict(PR_mdl,test)
?predict
# 
# cancer_tree <- C5.0(as.factor(PR.Status) ~ ., training)
# cancer_tree
# cancer_tree_pred <- predict(PR_mdl, test)

View(as_tibble(nhanes_tree_pred))


#Lets use the confusion matrix

confusionMatrix(as.factor(wine_predict), as.factor(test_w$text_rank), 
                dnn=c("Prediction", "Actual"), mode = "sens_spec")

table(test_w$text_rank)


wine_predict_p = predict(wine_mdl,test_w, type= "prob")



```


```{r, message=FALSE, echo = FALSE, warning=FALSE}
# 16 Summarize what you learned for each model along the way and make 
# recommendations to the world on how this could be used moving forward, 
# being careful not to over promise. 
```
